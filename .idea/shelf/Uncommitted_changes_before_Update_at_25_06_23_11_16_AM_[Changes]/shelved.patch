Index: main.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>#include \"includes.h\"\nusing namespace std;\n\nint main(int argc, char** argv) {\n    srand(time(NULL));\n\n    Network net;\n\n    layer_data input;\n    input.type = LAYER_NUM_INPUT;\n    input.n_out = {28, 28, 1};\n\n    layer_data convolutional;\n    convolutional.type = LAYER_NUM_CONVOLUTIONAL;\n    convolutional.stride_length = 1;\n    convolutional.receptive_field_length = 5;\n    convolutional.activationFunctPrime = reluPrime;\n    convolutional.activationFunct = relu;\n    convolutional.n_out = {-1,-1, 3};\n\n    layer_data maxpool;\n    maxpool.type = 2;\n    maxpool.summarized_region_length = 2;\n\n    layer_data fully_connected1;\n    fully_connected1.type = LAYER_NUM_FULLY_CONNECTED;\n    fully_connected1.activationFunctPrime = reluPrime;\n    fully_connected1.activationFunct = relu;\n    fully_connected1.n_out = {30, 1, 1};\n\n    layer_data fully_connected2;\n    fully_connected2.type = LAYER_NUM_FULLY_CONNECTED;\n    fully_connected2.activationFunctPrime = reluPrime;\n    fully_connected2.activationFunct = relu;\n    fully_connected2.n_out = {30, 1, 1};\n\n    layer_data outt;\n    outt.type = LAYER_NUM_FULLY_CONNECTED;\n    outt.activationFunctPrime = reluPrime;\n    outt.activationFunct = relu;\n    outt.last_layer = true;\n    outt.n_out = {10, 1, 1};\n\n    // FIND-TAG-231\n    int L = 6;\n    layer_data* layers = new layer_data[L];\n    layers[0] = input;\n    layers[1] = convolutional;\n    layers[2] = maxpool;\n    layers[3] = fully_connected1;\n    layers[4] = fully_connected2;\n    layers[5] = outt;\n\n    //cerr << \"ihr hurensÃ¶hne ich bin ein gotterbarmlicher hurensohn\\n\";\n\n    // train network\n    auto [test_data, test_data_size] = load_data(\"mnist_test_normalized.data\");\n    auto [training_data, training_data_size] = load_data(\"mnist_train_normalized.data\");\n\n    auto params = get_params();\n    if (argc == 7) {\n        params.fully_connected_weights_learning_rate = atof(argv[1]);\n        params.fully_connected_biases_learning_rate = atof(argv[2]);\n        params.convolutional_weights_learning_rate = atof(argv[3]);\n        params.convolutional_biases_learning_rate = atof(argv[4]);\n        params.L2_regularization_term = atof(argv[5]);\n        params.momentum_coefficient = atof(argv[6]);\n    }\n    params.test_data_size  = test_data_size;\n    params.training_data_size = training_data_size;\n    //cerr << \"epochs: \"; cin >> params.epochs;\n    params.epochs = 100;\n    net.init(layers, L, crossEntropyPrime, params);\n    net.SGD(training_data, test_data, params);\n\n    // TODO : watch this https://www.youtube.com/watch?v=m7E9piHcfr4 to make this faster\n    auto [correctTest, durationTest] = net.evaluate(test_data, test_data_size);\n    auto [correctTrain, durationTrain] = net.evaluate(training_data, training_data_size);\n\n    cout << \"accuracy in training data: \" << (float)correctTrain / params.training_data_size << \"\\n\";\n    cout << \"general accuracy: \" << (float)correctTest / params.test_data_size << \"\\n\";\n    //cout << (float)correctTest / params.test_data_size << \"\\n\";\n\n    net.save(argv[1]);\n\n    clear_data(test_data);\n    clear_data(training_data);\n    net.clear();\n    delete[] layers;\n}\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/main.cpp b/main.cpp
--- a/main.cpp	(revision e43d5b0083a6517c37af4266ad68762f40600874)
+++ b/main.cpp	(date 1687681866070)
@@ -69,7 +69,7 @@
     params.test_data_size  = test_data_size;
     params.training_data_size = training_data_size;
     //cerr << "epochs: "; cin >> params.epochs;
-    params.epochs = 100;
+    params.epochs = 1;
     net.init(layers, L, crossEntropyPrime, params);
     net.SGD(training_data, test_data, params);
 
@@ -87,4 +87,4 @@
     clear_data(training_data);
     net.clear();
     delete[] layers;
-}
+}
\ No newline at end of file
Index: layer.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>#include \"includes.h\"\n\nint get_convolutional_weights_index(int previous_map, int map, int y, int x, layer_data &data) {\n    return\n            previous_map * (data.n_out.feature_maps * data.receptive_field_length * data.receptive_field_length)\n            + map * (data.receptive_field_length * data.receptive_field_length)\n            + y * (data.receptive_field_length)\n            + x;\n}\n\nint get_fully_connected_weights_index(int neuron, int previous_neuron) {\n    return neuron*previous_neuron+previous_neuron;\n}\n\nint get_data_index(int map, int y, int x, layer_data &data) {\n    return\n            map * (data.n_out.x * data.n_out.y)\n            + y * (data.n_out.x)\n            + x;\n}\n\nrandom_device rd;\ndefault_random_engine generator(rd());\n\nvoid fully_connected_layer::init(layer_data data, layer_data data_previous) {\n\n    data.n_in = {data_previous.n_out.feature_maps * data_previous.n_out.y * data_previous.n_out.x, 1, 1};\n    this->data = data;\n    this->data_previous = data_previous;\n    normal_distribution<float> distribution(0.0, 1.0 / sqrt(data.n_in.x));\n\n    biases = new float [data.n_out.x];\n    biasesVelocity = new float [data.n_out.x];\n    for (int neuron = 0; neuron < data.n_out.x; neuron++) {\n        biases[neuron] = distribution(generator);\n        biasesVelocity[neuron] = 0;\n    }\n\n    weights = new float[data.n_out.x*data.n_in.x];\n    weightsVelocity = new float[data.n_out.x*data.n_in.x];\n    for (int neuron = 0; neuron < data.n_out.x; neuron++) {\n        for (int previous_neuron = 0; previous_neuron < data.n_in.x; previous_neuron++) {\n            weights[get_fully_connected_weights_index(neuron, previous_neuron)] = distribution(generator);\n            weightsVelocity[get_fully_connected_weights_index(neuron, previous_neuron)] = 0;\n        }\n    }\n\n    updateB = new float [data.n_out.x];\n    updateW = new float [data.n_out.x*data.n_in.x];\n    for (int bias = 0; bias < data.n_out.x; bias++) updateB[bias] = 0;\n    for (int weight = 0; weight < data.n_out.x*data.n_in.x; weight++) updateW[weight] = 0;\n}\n\nvoid fully_connected_layer::feedforward(float* a, float* dz, float* &new_a, float* &new_dz) {\n    (void) dz;\n    float* z = new float [data.n_out.x];\n    for (int i = 0; i < data.n_out.x; i++) z[i] = 0;\n\n    for (int neuron = 0; neuron < data.n_out.x; neuron++) {\n        // get new activations\n        for (int previous_neuron = 0; previous_neuron < data.n_in.x; previous_neuron++)\n            z[neuron] += weights[get_fully_connected_weights_index(neuron, previous_neuron)] * a[previous_neuron];\n        z[neuron] += biases[neuron];\n        new_a[neuron] = data.activationFunct(z[neuron]);\n        new_dz[neuron] = data.activationFunctPrime(z[neuron]);\n    }\n\n    delete[] z;\n}\n\nvoid\nfully_connected_layer::backprop(vector<float> &delta, float* &activations, float* &derivative_z) {\n\n    if (!data.last_layer) {\n        for (int neuron = 0; neuron < data.n_out.x; neuron++) delta[get_data_index(0, 0, neuron, data)] *= derivative_z[get_data_index(0, 0, neuron, data)];\n    }\n\n    for (int neuron = 0; neuron < data.n_out.x; neuron++) updateB[neuron] += delta[get_data_index(0, 0, neuron, data)];\n\n    vector<float> newDelta (data.n_in.x, 0);\n\n    for (int neuron = 0; neuron < data.n_out.x; neuron++) {\n        for (int previous_neuron = 0; previous_neuron < data.n_in.x; previous_neuron++) {\n            updateW[get_fully_connected_weights_index(neuron, previous_neuron)] += delta[get_data_index(0, 0, neuron, data)] * activations[get_data_index(0, 0, previous_neuron, data)];\n            newDelta[get_data_index(0, 0, previous_neuron, data_previous)] += delta[get_data_index(0, 0, neuron, data)] * weights[get_fully_connected_weights_index(neuron, previous_neuron)];\n        }\n    }\n    delta = newDelta;\n}\n\nvoid fully_connected_layer::update(hyperparams params) {\n    // update velocities\n    for (int neuron = 0; neuron < data.n_out.x; neuron++) {\n        biasesVelocity[neuron] = params.momentum_coefficient * biasesVelocity[neuron] -\n                                 (params.fully_connected_biases_learning_rate / params.mini_batch_size) *\n                                 updateB[neuron];\n    }\n\n    for (int neuron = 0; neuron < data.n_out.x; neuron++) {\n        for (int previous_neuron = 0; previous_neuron < data.n_in.x; previous_neuron++) {\n            weightsVelocity[get_fully_connected_weights_index(neuron, previous_neuron)] =\n                    params.momentum_coefficient * weightsVelocity[get_fully_connected_weights_index(neuron, previous_neuron)] -\n                    (params.fully_connected_weights_learning_rate / params.mini_batch_size) *\n                    updateW[get_fully_connected_weights_index(neuron, previous_neuron)];\n        }\n    }\n\n    // update weights and biases\n    for (int neuron = 0; neuron < data.n_out.x; neuron++) {\n        biases[neuron] += biasesVelocity[neuron];\n    }\n    for (int neuron = 0; neuron < data.n_out.x; neuron++) {\n        for (int previous_neuron = 0; previous_neuron < data.n_in.x; previous_neuron++) {\n            weights[get_fully_connected_weights_index(neuron, previous_neuron)] = (1 - params.fully_connected_weights_learning_rate *\n                                                    params.L2_regularization_term / params.training_data_size) *\n                                               weights[get_fully_connected_weights_index(neuron, previous_neuron)] +\n                                               weightsVelocity[get_fully_connected_weights_index(neuron, previous_neuron)];\n        }\n    }\n\n    for (int bias = 0; bias < data.n_out.x; bias++) updateB[bias] = 0;\n    for (int weight = 0; weight < data.n_out.x*data.n_in.x; weight++) updateW[weight] = 0;\n}\n\nvoid fully_connected_layer::save(string filename) {\n    ofstream file(filename, std::ios_base::app);\n\n    file << LAYER_NUM_FULLY_CONNECTED << \"//\";\n    file << data.n_out.x << \"//\";\n\n    for (int bias = 0; bias < data.n_out.x; bias++) file << biases[bias] << \" \";\n    file << \"//\";\n    for (int biasVel = 0; biasVel < data.n_out.x; biasVel++) file << biasesVelocity[biasVel] << \" \";\n    file << \"//\";\n    for (int weight = 0; weight < data.n_out.x*data.n_in.x; weight++) file << weights[weight] << \" \";\n    file << \"//\";\n    for (int weightVel = 0; weightVel < data.n_out.x*data.n_in.x; weightVel++) file << weightsVelocity[weightVel] << \" \";\n    file << \"\\n\";\n\n    file.close();\n}\n\nvoid fully_connected_layer::clear() {\n    delete[] weights;\n    delete[] weightsVelocity;\n    delete[] biases;\n    delete[] biasesVelocity;\n    delete[] updateW;\n    delete[] updateB;\n}\n\nvoid convolutional_layer::init(layer_data data, layer_data data_previous) {\n\n    data.n_in = data_previous.n_out;\n    data.n_out.x = (data.n_in.x - data.receptive_field_length + 1) / data.stride_length;\n    data.n_out.y = (data.n_in.y - data.receptive_field_length + 1) / data.stride_length;\n\n    this->data = data;\n    this->data_previous = data_previous;\n\n    weights_size = data.n_in.feature_maps * data.n_out.feature_maps * data.receptive_field_length * data.receptive_field_length;\n\n    normal_distribution<float> distribution(0.0, 1.0 / sqrt(data.receptive_field_length * data.receptive_field_length));\n\n    biases = new float[data.n_out.feature_maps];\n    biasesVelocity = new float[data.n_out.feature_maps];\n    for (int map = 0; map < data.n_out.feature_maps; map++) {\n        biases[map] = distribution(generator);\n        biasesVelocity[map] = 0;\n    }\n\n    weights = new float[weights_size];\n    weightsVelocity = new float[weights_size];\n    for (int previous_map = 0; previous_map < data.n_in.feature_maps; previous_map++) {\n        for (int map = 0; map < data.n_out.feature_maps; map++) {\n            for (int kernel_y = 0; kernel_y < data.receptive_field_length; kernel_y++) {\n                for (int kernel_x = 0; kernel_x < data.receptive_field_length; kernel_x++) {\n                    weights[get_convolutional_weights_index(previous_map, map, kernel_y, kernel_x, data)] = distribution(generator);\n                    weightsVelocity[get_convolutional_weights_index(previous_map, map, kernel_y, kernel_x, data)] = 0;\n                }\n            }\n        }\n    }\n\n    updateB = new float[data.n_out.feature_maps];\n    updateW = new float[weights_size];\n    for (int bias = 0; bias < data.n_out.feature_maps; bias++) updateB[bias] = 0;\n    for (int weight = 0; weight < weights_size; weight++) updateW[weight] = 0;\n}\n\nvoid convolutional_layer::feedforward(float* a, float* dz, float* &new_a, float* &new_dz) {\n    (void) dz;\n\n    float* z = new float [data.n_out.feature_maps * data.n_out.y * data.n_out.x];\n    for (int i = 0; i < data.n_out.feature_maps * data.n_out.y * data.n_out.x; i++) z[i] = 0;\n\n    //int rn = 0;\n\n    for (int map = 0; map < data.n_out.feature_maps; map++) {\n        for (int y = 0; y < data.n_out.y; y++) {\n            for (int x = 0; x < data.n_out.x; x++) {\n                for (int previous_map = 0; previous_map < data.n_in.feature_maps; previous_map++) {\n                    for (int kernel_y = 0; kernel_y < data.receptive_field_length; kernel_y++) {\n                        for (int kernel_x = 0; kernel_x < data.receptive_field_length; kernel_x++) {\n                            /*assert(get_convolutional_weights_index(previous_map, map, kernel_y, kernel_x, data) < weights_size);\n                            assert(get_data_index(previous_map, y * data.stride_length + kernel_y, x * data.stride_length + kernel_x, data_previous) < data_previous.n_out.feature_maps * data_previous.n_out.x * data_previous.n_out.y);\n                            if (weights[get_convolutional_weights_index(previous_map, map, kernel_y, kernel_x, data)] > 0) rn++;*/\n                            z[get_data_index(map, y, x, data)] +=\n                                    weights[get_convolutional_weights_index(previous_map, map, kernel_y, kernel_x, data)] *\n                                    a[get_data_index(previous_map, y * data.stride_length + kernel_y, x * data.stride_length + kernel_x, data_previous)];\n                        }\n                    }\n                }\n                //if (biases[map] > 0.5) rn++;\n                z[get_data_index(map, y, x, data)] += biases[map];\n                new_a[get_data_index(map, y, x, data)] = data.activationFunct(z[get_data_index(map, y, x, data)]);\n                new_dz[get_data_index(map, y, x, data)] = data.activationFunctPrime(z[get_data_index(map, y, x, data)]);\n            }\n        }\n    }\n\n    //cout << new_a[get_data_index(data.n_out.feature_maps-1, data.n_out.y-1, data.n_out.x-1, data)] << \" \";\n\n    delete[] z;\n}\n\nvoid convolutional_layer::backprop(vector<float> &delta,\n                                   float* &activations,\n                                   float* &derivative_z) {\n\n    for (int map = 0; map < data.n_out.feature_maps; map++) {\n        for (int y = 0; y < data.n_out.y; y++) {\n            for (int x = 0; x < data.n_out.x; x++) delta[get_data_index(map, y, x, data)] *= derivative_z[get_data_index(map, y, x, data)];\n        }\n    }\n\n    vector<float> newDelta(data.n_in.feature_maps * data.n_in.y *data.n_in.y, 0);\n\n    for (int map = 0; map < data.n_out.feature_maps; map++) {\n        for (int y = 0; y < data.n_out.y; y++) {\n            for (int x = 0; x < data.n_out.x; x++) {\n                updateB[map] += delta[get_data_index(map, y, x, data)];\n                for (int previous_map = 0; previous_map < data.n_in.feature_maps; previous_map++) {\n                    for (int kernel_y = 0; kernel_y < data.receptive_field_length; kernel_y++) {\n                        for (int kernel_x = 0; kernel_x < data.receptive_field_length; kernel_x++) {\n                            newDelta[get_data_index(previous_map, y * data.stride_length + kernel_y, x * data.stride_length +\n                                                                                      kernel_x, data_previous)] +=\n                                    delta[get_data_index(map, y, x, data)] * weights[get_convolutional_weights_index(previous_map, map, kernel_y, kernel_x, data)];\n                            updateW[get_convolutional_weights_index(previous_map, map, kernel_y, kernel_x, data)] +=\n                                    activations[get_data_index(previous_map, y * data.stride_length + kernel_y,\n                                            x * data.stride_length + kernel_x, data)] * delta[get_data_index(map, y, x, data)];\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    delta = newDelta;\n}\n\nvoid convolutional_layer::update(hyperparams params) {\n\n    for (int map = 0; map < data.n_out.feature_maps; map++) {\n        biasesVelocity[map] = params.momentum_coefficient * biasesVelocity[map] -\n                              (params.convolutional_biases_learning_rate / params.mini_batch_size) * updateB[map];\n        biases[map] += biasesVelocity[map];\n    }\n\n    for (int previous_map = 0; previous_map < data.n_in.feature_maps; previous_map++) {\n        for (int map = 0; map < data.n_out.feature_maps; map++) {\n            for (int kernel_y = 0; kernel_y < data.receptive_field_length; kernel_y++) {\n                for (int kernel_x = 0; kernel_x < data.receptive_field_length; kernel_x++) {\n                    weightsVelocity[get_convolutional_weights_index(previous_map, map, kernel_y, kernel_x, data)] =\n                            params.momentum_coefficient * weightsVelocity[get_convolutional_weights_index(previous_map, map, kernel_y, kernel_x, data)] -\n                            (params.convolutional_weights_learning_rate / params.mini_batch_size /\n                             (data.n_out.x * data.n_out.y) *\n                             data.stride_length * data.stride_length) * updateW[get_convolutional_weights_index(previous_map, map, kernel_y, kernel_x, data)];\n                    weights[get_convolutional_weights_index(previous_map, map, kernel_y, kernel_x, data)] = (1 -\n                                                                      params.convolutional_weights_learning_rate /\n                                                                      (data.n_out.x * data.n_out.y) *\n                                                                      data.stride_length * data.stride_length *\n                                                                      params.L2_regularization_term /\n                                                                      params.training_data_size) *\n                                                                     weights[get_convolutional_weights_index(previous_map, map, kernel_y, kernel_x, data)] +\n                                                                     weightsVelocity[get_convolutional_weights_index(previous_map, map, kernel_y, kernel_x, data)];\n                }\n            }\n        }\n    }\n\n    for (int i = 0; i < data.n_out.feature_maps; i++) updateB[i] = 0;\n    for (int i = 0; i < data.n_in.feature_maps * data.n_out.feature_maps * data.receptive_field_length * data.receptive_field_length; i++) updateW[i] = 0;\n}\n\nvoid convolutional_layer::save(string filename) {\n    ofstream file(filename, std::ios_base::app);\n\n    file << LAYER_NUM_CONVOLUTIONAL << \"//\";\n    file << data.stride_length << \" \" << data.receptive_field_length << \" \" << data.n_out.feature_maps << \"//\";\n\n    for (int bias = 0; bias < data.n_out.feature_maps; bias++) file << biases[bias] << \" \";\n    file << \"//\";\n    for (int biasVel = 0; biasVel < data.n_out.feature_maps; biasVel++) file << biasesVelocity[biasVel] << \" \";\n    file << \"//\";\n    for (int weight = 0; weight < weights_size; weight++) file << weights[weight] << \" \";\n    file << \"//\";\n    for (int weightVel = 0; weightVel < weights_size; weightVel++) file << weightsVelocity[weightVel] << \" \";\n    file << \"\\n\";\n\n    file.close();\n}\n\nvoid convolutional_layer::clear() {\n    delete[] weights;\n    delete[] weightsVelocity;\n    delete[] biases;\n    delete[] biasesVelocity;\n    delete[] updateW;\n    delete[] updateB;\n}\n\nvoid max_pooling_layer::init(layer_data data, layer_data data_previous) {\n    data.n_in = data_previous.n_out;\n    this->data = data;\n    this->data_previous = data_previous;\n    this->data.n_out.x = data.n_in.x / data.summarized_region_length;\n    this->data.n_out.y = data.n_in.y / data.summarized_region_length;\n    this->data.n_out.feature_maps = data_previous.n_out.feature_maps;\n}\n\nvoid max_pooling_layer::feedforward(float* a, float* dz, float* &new_a, float* &new_dz) {\n    (void) dz;\n\n    for (int i = 0; i < data.n_out.feature_maps * data.n_out.y * data.n_out.x; i++) new_a[i] = numeric_limits<float>::lowest();\n\n    for (int map = 0; map < data.n_out.feature_maps; map++) {\n        for (int y = 0; y < data.n_out.y; y++) {\n            for (int x = 0; x < data.n_out.x; x++) {\n                for (int kernel_y = 0; kernel_y < data.summarized_region_length; kernel_y++) {\n                    for (int kernel_x = 0; kernel_x < data.summarized_region_length; kernel_x++) {\n                        new_a[get_data_index(map, y, x, data)] = max(new_a[get_data_index(map, y, x, data)], a[get_data_index(map, y * data.summarized_region_length + kernel_y, x * data.summarized_region_length + kernel_x, data_previous)]);\n                    }\n                }\n                new_dz[get_data_index(map, y, x, data)] = new_a[get_data_index(map, y, x, data)];\n            }\n        }\n    }\n}\n\nvoid max_pooling_layer::backprop(vector<float> &delta,\n                                 float* &activations, float* &derivative_z) {\n    vector<float> newDelta(data.n_in.feature_maps * data.n_in.y * data.n_in.y, 0);\n    const float epsilon = 1e-8;\n\n    //cout << activations[get_data_index(data.n_out.feature_maps-1, (data.n_out.y-1)*data.summarized_region_length+data.summarized_region_length-1, (data.n_out.x-1)*data.summarized_region_length+data.summarized_region_length-1, data_previous)] << \"sdfkjdslksfjlsf\\n\";\n    for (int map = 0; map < data.n_out.feature_maps; map++) {\n        for (int y = 0; y < data.n_out.y; y++) {\n            for (int x = 0; x < data.n_out.x; x++) {\n                for (int kernel_y = 0; kernel_y < data.summarized_region_length; kernel_y++) {\n                    for (int kernel_x = 0; kernel_x < data.summarized_region_length; kernel_x++) {\n                        int act = activations[get_data_index(map, y * data.summarized_region_length + kernel_y, x * data.summarized_region_length + kernel_x, data_previous)];\n                        int dev = derivative_z[get_data_index(map, y, x, data)];\n                        if (act < dev) swap(act, dev);\n                        if (act - dev < epsilon) {\n                            newDelta[get_data_index(map, y * data.summarized_region_length + kernel_y,\n                                    x * data.summarized_region_length + kernel_x, data_previous)] = delta[get_data_index(map, y, x, data)];\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    delta = newDelta;\n}\n\nvoid max_pooling_layer::update(hyperparams params) {\n    (void) params;\n}\n\nvoid max_pooling_layer::save(string filename) {\n    ofstream file(filename, std::ios_base::app);\n\n    file << LAYER_NUM_MAX_POOLING << \"//\";\n    file << data.summarized_region_length << \"\\n\";\n\n    file.close();\n}\n\nvoid max_pooling_layer::clear() {}\n\nvoid input_layer::init(layer_data data, layer_data data_previous) {\n    this->data = data;\n    (void) data_previous;\n}\n\nvoid input_layer::feedforward(float* a, float* dz, float* &new_a, float* &new_dz) {\n    (void) a;\n    (void) dz;\n    (void) new_a;\n    (void) new_dz;\n}\n\nvoid input_layer::backprop(vector<float> &delta,\n                           float* &activations, float* &derivative_z) {\n    (void) delta;\n    (void) activations;\n    (void) derivative_z;\n}\n\nvoid input_layer::update(hyperparams params) {\n    (void) params;\n}\n\nvoid input_layer::save(string filename) {\n    ofstream file(filename, std::ios_base::app);\n\n    file << LAYER_NUM_INPUT << \"//\";\n    file << data.n_out.x << \" \" << data.n_out.y << \"\\n\";\n\n    file.close();\n}\n\nvoid input_layer::clear() {}
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/layer.cpp b/layer.cpp
--- a/layer.cpp	(revision e43d5b0083a6517c37af4266ad68762f40600874)
+++ b/layer.cpp	(date 1687634225083)
@@ -353,7 +353,6 @@
     vector<float> newDelta(data.n_in.feature_maps * data.n_in.y * data.n_in.y, 0);
     const float epsilon = 1e-8;
 
-    //cout << activations[get_data_index(data.n_out.feature_maps-1, (data.n_out.y-1)*data.summarized_region_length+data.summarized_region_length-1, (data.n_out.x-1)*data.summarized_region_length+data.summarized_region_length-1, data_previous)] << "sdfkjdslksfjlsf\n";
     for (int map = 0; map < data.n_out.feature_maps; map++) {
         for (int y = 0; y < data.n_out.y; y++) {
             for (int x = 0; x < data.n_out.x; x++) {
Index: CMakeLists.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>cmake_minimum_required(VERSION 3.16)\nproject(neuralnetwork)\n\nset(CMAKE_CXX_STANDARD 17)\n\nadd_compile_options(-Wall -Wextra -std=c++17)\nadd_definitions(-funroll-loops -Ofast)\n\nadd_executable(neuralnetwork main.cpp Network.cpp layer.cpp misc.cpp)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/CMakeLists.txt b/CMakeLists.txt
--- a/CMakeLists.txt	(revision e43d5b0083a6517c37af4266ad68762f40600874)
+++ b/CMakeLists.txt	(date 1687634187507)
@@ -4,6 +4,6 @@
 set(CMAKE_CXX_STANDARD 17)
 
 add_compile_options(-Wall -Wextra -std=c++17)
-add_definitions(-funroll-loops -Ofast)
+#add_definitions(-funroll-loops -Ofast)
 
 add_executable(neuralnetwork main.cpp Network.cpp layer.cpp misc.cpp)
